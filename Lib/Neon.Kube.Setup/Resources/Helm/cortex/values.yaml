# Configuration for running Cortex in single-process mode.
# This should not be used in production.  It is only for getting started
# and development.

image:
  organization: neon-registry.node.local
  repository: cortexproject-cortex
  tag: v1.11.0
  pullPolicy: IfNotPresent

serviceAccount:
  create: true
  name:
  annotations: {}

rbac:
  create: true
  pspEnabled: true

replicas: 1

podManagementPolicy: Parallel

updateStrategy:
  type: RollingUpdate

serviceMesh:
  enabled: true

cortexConfig:
  target: all,alertmanager,compactor
  auth_enabled: true
  http_prefix: /api/prom
  api:
    response_compression_enabled: true
    alertmanager_http_prefix: /alertmanager
    prometheus_http_prefix: /prometheus
  server:
    http_listen_port: 9009
    grpc_listen_port: 9095
    # Configure the server to allow messages up to 100MB.
    grpc_server_min_time_between_pings: 1s
    grpc_server_ping_without_stream_allowed: true
  distributor:
    remote_timeout: 10s
    extend_writes: true
    shard_by_all_labels: true
    pool:
      health_check_ingesters: true
    ha_tracker:
      enable_ha_tracker: false
      kvstore:
        store: etcd
        prefix: neonkube.io/cortex/ha-tracker/
        etcd:
          endpoints:
            - neon-etcd.neon-system.svc.cluster.local:2379
    ring:
      kvstore:
        store: etcd
        prefix: neonkube.io/cortex/collectors/
  ingester_client:
    grpc_client_config:
      grpc_compression: gzip
  storage:
    engine: blocks
    index_queries_cache_config:
      redis:
        db: 4
        endpoint: neon-redis.neon-system.svc.cluster.local:26379
        master_name: master
        timeout: 2000ms
  limits:
    enforce_metric_name: true
    reject_old_samples: true
    reject_old_samples_max_age: 1h
    max_query_lookback: 0s
    max_series_per_metric: 0
    max_series_per_user: 0
    max_metadata_per_user: 0
    max_metadata_per_metric: 100
    compactor_blocks_retention_period: 6h
  blocks_storage:
    backend: s3
    tsdb:
      dir: /tmp/cortex/tsdb
      block_ranges_period:
        - 30m0s
      retention_period: 1h30m0s
      stripe_size: 4096
      wal_compression_enabled: true
    bucket_store:
      max_concurrent: 25
      max_chunk_pool_bytes: 357913941
      sync_dir: /tmp/cortex/tsdb-sync
      block_sync_concurrency: 5
      meta_sync_concurrency: 5
      index_cache:
        backend: inmemory
        inmemory:
          max_size_bytes: 357913941
    s3:
      bucket_name: cortex
      endpoint: minio.neon-system
      access_key_id: ${ACCESS_KEY_ID}
      secret_access_key: ${SECRET_ACCESS_KEY}
      insecure: true
  ingester:
    retain_period: 1m
    max_chunk_age: 1h
    lifecycler:
      unregister_on_shutdown: true
      # The address to advertise for this ingester. Will be autodiscovered by
      # looking up address on eth0 or en0; can be specified if this fails.
      min_ready_duration: 20s
      #readiness_check_ring_health: false
      # We want to start immediately and flush on shutdown.
      join_after: 5s
      final_sleep: 30s
      num_tokens: 128

      # Use an in memory ring store, so we don't need to launch a Consul.
      ring:
        replication_factor: 1
        kvstore:
          store: etcd
          prefix: neonkube.io/cortex/collectors/
          etcd:
            endpoints:
              - neon-etcd.neon-system.svc.cluster.local:2379
  frontend_worker:
    frontend_address: 0.0.0.0:9095
  alertmanager:
    data_dir: /tmp/cortex/alertmanager
    fallback_config_file: /etc/cortex/alertmanager.yaml
    cluster:
      listen_address: 0.0.0.0:9094
    enable_api: true
    external_url: /api/prom/alertmanager
    sharding_enabled: true
    sharding_ring:
      replication_factor: 1
      kvstore:
        store: etcd
        prefix: neonkube.io/cortex/alertmanagers/
        etcd:
          endpoints:
            - neon-etcd.neon-system.svc.cluster.local:2379
  alertmanager_storage:
    backend: s3
    s3:
      access_key_id: ${ACCESS_KEY_ID}
      bucket_name: alertmanager
      endpoint: minio.neon-system
      insecure: true
      secret_access_key: ${SECRET_ACCESS_KEY}
  querier:
    batch_iterators: true
    store_gateway_addresses: dns+cortex.neon-monitor.svc.cluster.local:9095
    active_query_tracker_dir: /tmp/cortex/query-tracker
  query_range:
    align_queries_with_step: true
    cache_results: true
    results_cache:
      cache:
        redis:
          db: 3
          endpoint: neon-redis.neon-system.svc.cluster.local:26379
          master_name: master
          timeout: 2000ms
    split_queries_by_interval: 24h
  ruler:
    alertmanager_url: /api/prom/alertmanager
    external_url: /api/prom/ruler
    enable_alertmanager_v2: true
    enable_api: true
    enable_sharding: true
    ring:
      kvstore:
        store: etcd
        prefix: neonkube.io/cortex/rulers/
        etcd:
          endpoints:
            - neon-etcd.neon-system.svc.cluster.local:2379
    rule_path: /tmp/cortex/rules
  ruler_storage:
    backend: s3
    s3:
      access_key_id: ${ACCESS_KEY_ID}
      bucket_name: cortex-ruler
      endpoint: minio.neon-system
      insecure: true
      secret_access_key: ${SECRET_ACCESS_KEY}
  compactor:
    block_ranges:
      - 30m0s
      - 2h0m0s
      - 12h0m0s
      - 24h0m0s
    meta_sync_concurrency: 5
    block_sync_concurrency: 5
    compaction_retries: 1
    cleanup_interval: 15m
    deletion_delay: 1h
    tenant_cleanup_delay: 1h
    data_dir: /tmp/cortex/compactor
    sharding_enabled: true
    sharding_ring:
      kvstore:
        store: etcd
        prefix: neonkube.io/cortex/collectors/
        etcd:
          endpoints:
            - neon-etcd.neon-system.svc.cluster.local:2379
  purger:
    enable: true
    delete_request_cancel_period: 5m
    object_store_type: s3
  table_manager:
    retention_deletes_enabled: true
    retention_period: 24h
  store_gateway:
    sharding_enabled: true
    sharding_ring:
      kvstore:
        store: etcd
        prefix: neonkube/cortex/collectors/
        etcd:
          endpoints:
            - neon-etcd.neon-system.svc.cluster.local:2379

strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 0
    maxUnavailable: 1

annotations:
  reloader.stakater.com/auto: "true"
nodeSelector:
  neonkube.io/monitor.metrics-internal: 'true'

affinity: {}

tolerations:
  - key: "neonkube.io/metrics"
    operator: Exists
    effect: NoSchedule
  - key: "neonkube.io/metrics-internal"
    operator: Exists
    effect: NoSchedule

resources: {}

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "http-metrics"
  readiness.status.sidecar.istio.io/applicationPorts: '8080,9095'
  traffic.sidecar.istio.io/excludeInboundPorts: '2379'
  traffic.sidecar.istio.io/excludeOutboundPorts: '2379'

podLabels: {}

terminationGracePeriodSeconds: 2400

extraVolumes: {}

env:
  - name: MY_POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        name: minio
        key: accesskey
  - name: SECRET_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        name: minio
        key: secretkey
  - name: GOGC
    value: "10"

securityContext:
  fsGroup: 1000
  runAsGroup: 1000
  runAsNonRoot: true
  runAsUser: 1000

initContainers: []

livenessProbe:
  httpGet:
    path: /ready
    port: http-metrics
  initialDelaySeconds: 60
  failureThreshold: 100
  periodSeconds: 30
readinessProbe:
  httpGet:
    path: /ready
    port: http-metrics
  initialDelaySeconds: 60
  failureThreshold: 60
  periodSeconds: 10

serviceMonitor:
  enabled: true
  interval: 60s
